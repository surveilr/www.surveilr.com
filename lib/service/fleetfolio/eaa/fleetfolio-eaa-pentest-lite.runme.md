---
runme:
  id: fleetfolio-eaa-pentest-lite
  name: "Fleetfolio EAA – Pentest Lite (Authorized)"
  version: "2.0.0"
  maintainer: "Fleetfolio Team"
  description: "Lightweight, authorized, container-run external asset assessment with structured artifacts."
---

This Runme runbook performs an **authorized, lightweight external asset assessment** inside an OWASP Nightingale container
(default base: `rajanagori/nightingale:latest`). It orchestrates a chain of reconnaissance and validation tools to
discover domains, resolve DNS, probe for web services, scan ports, fingerprint technologies, check TLS certificates, and run
targeted vulnerability checks. All artifacts are stored under `/var/fleetfolio/eaa/<tool>/…` in structured formats such as
JSON, JSONL, XML, or plain text.

The runbook is meant to be **safe, idempotent, and scope-aware**, honoring exclusions and customer authorization.

All the packages come from `Debian`. When they don’t come from Debian, we use `Homebrew`. If not available in Homebrew, we use `eget`. If none of these work, we install it directly using the runtime installation instructions.

## Roadmap

- [ ] Introduce [Masscan](https://www.kali.org/tools/masscan/) for High-speed Internet-scale port scanning with banner grabbing
- [ ] Introduce [AutoRecon](https://github.com/Tib3rius/AutoRecon) for Comprehensive automated reconnaissance with 35+ parameters
- [ ] Introduce [Fierce](https://github.com/mschwager/fierce) for DNS reconnaissance and zone transfer testing

## Manual Installation of Kali Linux in VirtualBox

This guide will help you manually install **Kali Linux** in **VirtualBox** on your host machine.

### Prerequisites

- **VirtualBox**: [Download VirtualBox](https://www.virtualbox.org/wiki/Downloads)
- If you are using Windows as the host machine, you may encounter an error while downloading or installing VirtualBox due to missing Visual C++ dependencies. To resolve this, download the package from [Visual C++ Redistributable Runtimes All-in-One](https://www.techpowerup.com/download/visual-c-redistributable-runtime-package-all-in-one)
   and run the `install_all.bat` file.
- **Kali Linux Virtual Machine**: [Download Kali Linux VM](https://www.kali.org/get-kali/#kali-platforms) (VirtualBox, 3.3 GB, `.7z` file)
- **WinRAR** or **7-Zip** for extraction: [Download WinRAR](https://www.win-rar.com/download.html?&L=0) (recommended)

#### Steps

1. **Download Kali Linux**

   - Select **Virtual Machine** and then **VirtualBox (3.3 GB)**.
   - The file will be in `.7z` format.

2. **Download and install VirtualBox**

   - Open [VirtualBox Downloads](https://www.virtualbox.org/wiki/Downloads) and install it on your host machine.

3. **Extract Kali Linux VM**

   - Use **WinRAR** or **7-Zip** to extract the `.7z` file to your **Downloads** folder.
   - There will be two files: A `.vdi` file (orange icon) and one `.vbox file` (blue icon). Double-click on the `.vbox` file to automatically import the Kali Linux Machine into VirtualBox.

4. **Configure the Virtual Machine**

   - Set **Base Memory** to `4096 MB` (recommended).
   - Set **Processor Cores** to `2–3` (recommended).
   - Start the virtual machine.

5. **Login**

- Default credentials:

```text { ignore=true }
Username: kali
Password: kali
```

6. **Switch to Root User**

- Open the terminal and type:

```bash { ignore=true }
sudo su
```

- This will give you root privileges (most privileged user).

7. **Update the System**

- In the terminal, run:

```bash { ignore=true }
sudo apt update && sudo apt upgrade -y && sudo apt full-upgrade -y
```

- This will ensure your system is fully updated and prevent errors when installing tools.

**Note:** Always use updated VirtualBox and Kali Linux versions to avoid compatibility issues.

## Fixing Errors

If you encounter any errors while running the above system update commands, follow these steps:

```bash { ignore=true }
sudo sed -i 's|http://http.kali.org|https://http.kali.org|' /etc/apt/sources.list
```

Then run the following commands one by one:

```bash { ignore=true }
sudo apt-get clean && sudo apt-get update --fix-missing
```

Then run the system update command again:

```bash { ignore=true }
sudo apt update && sudo apt upgrade -y && sudo apt full-upgrade -y
```

---

### Prerequisite Dependency

**Homebrew**

```bash { ignore=true }
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)" && echo >> /home/kali/.zshrc && echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> /home/kali/.zshrc && eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" && sudo apt-get install -y build-essential && brew install gcc
```

Note : Run this command inside a non-root user terminal. You will be prompted to enter the password, and then press Enter again to continue

**Go**

```bash { ignore=true }
sudo apt install golang-go -y
```

---

#### Install [RUNME](https://github.com/runmedev/runme)

To get started with Runme on Linux, the recommended method of installation is using Homebrew, a popular package manager that simplifies the installation process. First, ensure your Homebrew is up to date to avoid any compatibility issues:

```bash { ignore=true }
brew update && brew install runme
```

#### Install [Varlock](https://github.com/dmno-dev/varlock)

Varlock is a tool for managing and validating environment variables using a schema file, ensuring all required variables are correctly set before running commands. It adds consistency, security, and reliability to workflows that depend on .env files.
To install:

```bash { ignore=true }
brew install dmno-dev/tap/varlock
```

Homebrew-installed tools not accessible when switching to root user (Solution: Add Homebrew to root’s shell environment)

```bash { ignore=true }
sudo su && echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> /root/.zshrc && eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
```

#### Install [Surveilr](https://github.com/opsfolio/releases.opsfolio.com/releases)

Get the latest Resource Surveillance & Integration Engine (surveilr) for Critical Systems by following these steps to complete the installation:

```bash { ignore=true }
sudo apt install wget &&  wget https://github.com/opsfolio/releases.opsfolio.com/releases/download/2.2.0/resource-surveillance_2.2.0_x86_64-unknown-linux-gnu.tar.gz && \
tar -xvf resource-surveillance_2.2.0_x86_64-unknown-linux-gnu.tar.gz && \
mv surveilr /usr/local/bin
```

## Tools Required and Their Installation Steps

#### Whatweb, Nmap, Openssl, wafw00f, sqlmap, WPScan, Amass, Nikto, DNSEnum, TheHarvester are Pre-installed in kali linux

```bash { ignore=true }
sudo apt install -y subfinder dnsx httpx-toolkit naabu nuclei xq jq dirsearch testssl.sh paramspider && \
git clone https://github.com/s0md3v/Corsy.git && cd Corsy && pip3 install -r requirements.txt --break-system-packages
go install github.com/projectdiscovery/katana/cmd/katana@latest && sudo cp ~/go/bin/katana /bin/  && \
go install github.com/projectdiscovery/tlsx/cmd/tlsx@latest && sudo cp ~/go/bin/tlsx /bin/ && \
go install -v github.com/PentestPad/subzy@latest && sudo cp ~/go/bin/subzy /bin/ && \
git clone https://github.com/r0oth3x49/ghauri.git && cd ghauri && sudo python3 setup.py install && \
brew install rustscan dalfox
```

## Then Clone this repo

```bash { ignore=true }
git clone https://github.com/surveilr/www.surveilr.com.git && \
cd www.surveilr.com/lib/service/fleetfolio/eaa
```

- Then add the domains, URLs, IPs, etc. inside the .env file.
   Example

```bash { ignore=true }
sudo tee -a .env > /dev/null << 'EOF'

# Copy this file to ".env" and adjust values as needed.
# docker-compose will automatically read .env in the same folder.

# Scope configuration (space- or comma-separated)
OPSFOLIO_EAA_HOME=/opt/eaa/sessions
OPSFOLIO_EAA_TENANT_ID=NET1234
OPSFOLIO_EAA_TENANT_NAME=Netspective
OPSFOLIO_EAA_PURPOSE=Threat
OPSFOLIO_EAA_DESCRIPTION=Demo_Threat
OPSFOLIO_EAA_CONTEXT_JSON='{ "Sample": "Value" }'
OPSFOLIO_EAA_DOMAINS=netspective.com
OPSFOLIO_EAA_SUBDOMAINS=
OPSFOLIO_EAA_IP_RANGES=
OPSFOLIO_EAA_EXCLUDES=
OPSFOLIO_EAA_KEY_URLS=https://netspective.com/
OPSFOLIO_EAA_CORSY_PATH=/opt/eaa/Corsy
OPSFOLIO_EAA_WP_TOKEN=TOKEN
OPSFOLIO_EAA_RATE_LIMIT=200
OPSFOLIO_EAA_CONCURRENCY=50
OPSFOLIO_EAA_NAABU_PORTS=top-100
OPSFOLIO_EAA_NUCLEI_TEMPLATES=cves,default

EOF
```

- Then follow these steps

## Run Varlock with Runme to Execute Fleetfolio Pentest Workflow

```bash { ignore=true }
varlock init && varlock load && varlock run -- runme run --filename="fleetfolio-eaa-pentest-lite.runme.md" --all
```

Note: Make sure you are the root user and inside the eaa directory before running the above command. Then, press "y" twice to ensure that the environment variables are declared by Varlock

---

## Automated Method (Docker)

- First, download Docker. If Docker is already installed, you can skip this step. Ensure that Docker is running by checking `sudo systemctl status docker`. If it is not running, enable and start it with `sudo systemctl enable docker --now`.

```bash { ignore=true }
sudo apt update && sudo apt install docker.io -y && sudo systemctl enable docker --now
```

If you encounter any errors while running the above system update commands, follow these steps:

```bash { ignore=true }
sudo sed -i 's|http://http.kali.org|https://http.kali.org|' /etc/apt/sources.list && sudo apt-get clean && sudo apt-get update --fix-missing
```

- Then Clone this repo

```bash { ignore=true }
git clone https://github.com/surveilr/www.surveilr.com.git && \
cd www.surveilr.com/lib/service/fleetfolio/eaa
```

- Then add the domains, URLs, IPs, etc. inside the .env file.
   Example

```bash { ignore=true }
sudo tee -a .env > /dev/null << 'EOF'

# Copy this file to ".env" and adjust values as needed.
# docker-compose will automatically read .env in the same folder.

# Scope configuration (space- or comma-separated)
OPSFOLIO_EAA_HOME=/opt/eaa/sessions
OPSFOLIO_EAA_TENANT_ID=NET1234
OPSFOLIO_EAA_TENANT_NAME=Netspective
OPSFOLIO_EAA_PURPOSE=Threat
OPSFOLIO_EAA_DESCRIPTION=Demo_Threat
OPSFOLIO_EAA_CONTEXT_JSON='{ "Sample": "Value" }'
OPSFOLIO_EAA_DOMAINS=netspective.com
OPSFOLIO_EAA_SUBDOMAINS=
OPSFOLIO_EAA_IP_RANGES=
OPSFOLIO_EAA_EXCLUDES=
OPSFOLIO_EAA_KEY_URLS=https://netspective.com/
OPSFOLIO_EAA_CORSY_PATH=/opt/eaa/Corsy
OPSFOLIO_EAA_WP_TOKEN=TOKEN
OPSFOLIO_EAA_RATE_LIMIT=200
OPSFOLIO_EAA_CONCURRENCY=50
OPSFOLIO_EAA_NAABU_PORTS=top-100
OPSFOLIO_EAA_NUCLEI_TEMPLATES=cves,default

EOF
```

- Then follow these steps

```bash { ignore=true }
sudo docker build -t fleetfolio-eaa:latest . && \
sudo docker run --rm -it -v $(pwd)/results:/opt/eaa/sessions/ fleetfolio-eaa:latest
```

Note: After running the `docker run` command, a new directory called `results` will be created in the current path on your local machine. This directory will contain the generated results from the Docker container, based on the scope you provided.

## Environment Variables

- `OPSFOLIO_EAA_HOME` – (default: `opt/eaa/sessions`)
- `OPSFOLIO_EAA_TENANT_ID` – tenant IDs, separated by commas or spaces
- `OPSFOLIO_EAA_TENANT_NAME` – tenant names, separated by commas or spaces
- `OPSFOLIO_EAA_PURPOSE`   –    required string
- `OPSFOLIO_EAA_DESCRIPTION` –   optional string
- `OPSFOLIO_EAA_CONTEXT_JSON`   –     optional JSON object
- `OPSFOLIO_EAA_DOMAINS` – comma- or space-separated domains
- `OPSFOLIO_EAA_IP_RANGES` – comma- or space-separated IPs/CIDRs
- `OPSFOLIO_EAA_KEY_URLS` – comma- or space-separated URLs/APIs
- `OPSFOLIO_EAA_EXCLUDES` – comma- or space-separated exclusions
- `OPSFOLIO_EAA_CORSY_PATH` – (default: `/opt/eaa/Corsy`)
- `OPSFOLIO_EAA_WP_TOKEN` – required string
- `OPSFOLIO_EAA_RATE_LIMIT` – (default: 200)
- `OPSFOLIO_EAA_CONCURRENCY` – (default: 50)
- `OPSFOLIO_EAA_NAABU_PORTS` – (default: top-100)
- `OPSFOLIO_EAA_NUCLEI_TEMPLATES` – (default: cves,default)

# Initialization and Logging

The runbook begins by creating a working directory for session data, evidence, and logs. It snapshots all Fleetfolio-related
environment variables so that the test configuration is preserved for auditability. The master log file is written to
`$OPSFOLIO_EAA_HOME/runbook.log`.

```bash { name=init }
# Generate timestamp for the session folder
timestamp=$(date +'%Y-%m-%d-%H-%M-%S')

# Define dynamic output directory
export OPSFOLIO_EAA_HOME="/opt/eaa/sessions/$timestamp"
export OPSFOLIO_EAA_SESSION_HOME="$OPSFOLIO_EAA_HOME/.session"

# Create the necessary directories
mkdir -p "$OPSFOLIO_EAA_SESSION_HOME"

# Initialize log and arguments file inside the timestamped session directory
LOG="$OPSFOLIO_EAA_SESSION_HOME/runbook.log"
(env | grep ^OPSFOLIO_EAA_ || true) | while read -r line; do
    echo "$line" | sed 's/\(.*\)=\(.*\)/\1="\2"/' >> "$OPSFOLIO_EAA_SESSION_HOME/arguments.env"
done
echo "[init] Snapshot written to $OPSFOLIO_EAA_SESSION_HOME/arguments.env" | tee -a "$LOG"
```

Artifacts: `$OPSFOLIO_EAA_SESSION_HOME/arguments.env` (environment variables in `key=value` format) and the master runbook log
at `$OPSFOLIO_EAA_SESSION_HOME/runbook.log`.

# Scope Normalization

Environment variables are convenient but not ideal for tool chaining. Here we normalize them into newline-delimited files so
each tool can consume scope easily.

```bash { name=scope }
# Ensure .env is loaded from the correct location
if [ -f .env ]; then
  echo "[INFO] .env file found in current dir, loading..."
  set -a
  . ./.env
  set +a
elif [ -f /opt/eaa/.env ]; then
  echo "[INFO] .env file found in /opt/eaa/, loading..."
  set -a
  . /opt/eaa/.env
  set +a
else
  echo "[ERROR] .env file not found!"
  exit 1
fi

# Reuse the timestamp from the environment snapshot created earlier
if [ -z "${timestamp:-}" ] && [ -f "$OPSFOLIO_EAA_SESSION_HOME/arguments.env" ]; then
  # Load environment from first snippet
  set -a
  . "$OPSFOLIO_EAA_SESSION_HOME/arguments.env"
  set +a
fi

# Ensure OPSFOLIO_EAA_HOME is set correctly
: "${OPSFOLIO_EAA_HOME:?OPSFOLIO_EAA_HOME must be set in .env}"
export OPSFOLIO_EAA_SESSION_HOME="$OPSFOLIO_EAA_HOME/.session"

# Create the session folder in the timestamped directory
mkdir -p "$OPSFOLIO_EAA_SESSION_HOME"

# Copy runbook to the session folder
env > "$OPSFOLIO_EAA_SESSION_HOME/.env"
cp fleetfolio-eaa-pentest-lite.runme.md "$OPSFOLIO_EAA_SESSION_HOME"
cp -r .interpretation-template "$OPSFOLIO_EAA_SESSION_HOME"

# Normalize scope variables into text files inside the .session folder
echo "${OPSFOLIO_EAA_TENANT_ID:-}"   | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/tenant_id.txt"
echo "${OPSFOLIO_EAA_TENANT_NAME:-}" | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/tenant_name.txt"
echo "${OPSFOLIO_EAA_PURPOSE:-}"   | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/purpose.txt"
echo "${OPSFOLIO_EAA_DESCRIPTION:-}" | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/description.txt"
echo "${OPSFOLIO_EAA_CONTEXT_JSON:-}"   | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/context.json"
echo "${OPSFOLIO_EAA_DOMAINS:-}"   | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/domains.txt"
echo "${OPSFOLIO_EAA_SUBDOMAINS:-}"| tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/subdomains.txt"
echo "${OPSFOLIO_EAA_CORSY_PATH:-}"| tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/corsy-path.txt"
echo "${OPSFOLIO_EAA_WP_TOKEN:-}"| tr ', ' '\n' | sed '/^$/d' >  "$OPSFOLIO_EAA_SESSION_HOME/wp_token.txt"
echo "${OPSFOLIO_EAA_IP_RANGES:-}" | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/ip_ranges.txt"
echo "${OPSFOLIO_EAA_KEY_URLS:-}"  | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt"
echo "${OPSFOLIO_EAA_EXCLUDES:-}"  | tr ', ' '\n' | sed '/^$/d' > "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt"
```

Artifacts: text files under `$OPSFOLIO_EAA_SESSION_HOME` for domains, IP ranges, key URLs, and excludes.

## Scope and Normalization Using LHC-Forms

- Suppose you have an LHC Form; run this script to convert the LHC Form into environment variables.

```bash { ignore=true }
# Load jq to parse JSON (make sure jq is installed)
if ! command -v jq &> /dev/null
then
    echo "jq not found, please install jq to proceed."
    exit 1
fi

# File path
json_file="assessment.lhc-form.json"

# Extract required values from JSON using jq
TENANT_NAME=$(jq -r '.items[0].items[0].value' "$json_file")
DOMAINS=$(jq -r '.items[1].items[0].value' "$json_file")
IP_RANGES=$(jq -r '.items[2].items[0].value' "$json_file")
KEY_URLS=$(jq -r '.items[3].items[0].value' "$json_file")

# Export the values as environment variables
export OPSFOLIO_EAA_TENANT_NAME="$TENANT_NAME"
export OPSFOLIO_EAA_DOMAINS="$DOMAINS"
export OPSFOLIO_EAA_IP_RANGES="$IP_RANGES"
export OPSFOLIO_EAA_KEY_URLS="$KEY_URLS"

# Output the values (optional for verification)
echo "OPSFOLIO_EAA_TENANT_NAME: $OPSFOLIO_EAA_TENANT_NAME"
echo "OPSFOLIO_EAA_DOMAINS: $OPSFOLIO_EAA_DOMAINS"
echo "OPSFOLIO_EAA_IP_RANGES: $OPSFOLIO_EAA_IP_RANGES"
echo "OPSFOLIO_EAA_KEY_URLS: $OPSFOLIO_EAA_KEY_URLS"
```

# Subfinder – Discovering Subdomains

[Subfinder](https://github.com/projectdiscovery/subfinder) A subdomain discovery tool that uses passive online sources to quickly enumerate subdomains for a given domain. It’s widely used in reconnaissance to map the attack surface.  
**Use Cases:**

- Discovering subdomains of a target organization during reconnaissance.
- Expanding attack surface before vulnerability scanning.
- Validating scope in bug bounty programs.

```bash { name=subfinder }

# Define output path for subfinder
OUT="$OPSFOLIO_EAA_HOME/subfinder/subfinder.jsonl"
mkdir -p "$(dirname "$OUT")"

DOMAINS_FILE="$OPSFOLIO_EAA_SESSION_HOME/domains.txt"
SUBDOMAINS_FILE="$OPSFOLIO_EAA_SESSION_HOME/subdomains.txt"

# Prepare input files if they exist in .env
if [ -n "${OPSFOLIO_EAA_DOMAINS:-}" ]; then
  echo "$OPSFOLIO_EAA_DOMAINS" | tr ' ,;' '\n' > "$DOMAINS_FILE"
fi

if [ -n "${OPSFOLIO_EAA_SUBDOMAINS:-}" ]; then
  echo "$OPSFOLIO_EAA_SUBDOMAINS" | tr ' ,;' '\n' > "$SUBDOMAINS_FILE"
fi

# Case 1: Domains present → run subfinder as usual
if [ -s "$DOMAINS_FILE" ]; then
  while read -r DOMAIN; do
    BASE=$(echo "$DOMAIN" | awk -F. '{print $(NF-1)"."$NF}')
    if [ "$DOMAIN" = "$BASE" ]; then
      subfinder -d "$DOMAIN" -oJ -silent >> "$OUT" || true
    else
      echo "$DOMAIN" | dnsx -resp -silent | jq -R '{host:.}' >> "$OUT" || true
    fi
  done < "$DOMAINS_FILE"

# Case 2: No domains, but subdomains provided → just save them
elif [ -s "$SUBDOMAINS_FILE" ]; then
  while read -r SUB; do
    echo "$SUB" | jq -R '{host:.}' >> "$OUT"
  done < "$SUBDOMAINS_FILE"
fi
```

Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/subfinder/subfinder.jsonl`, one JSON record per subdomain discovered.

# dnsx – Resolving Hosts

[dnsx](https://github.com/projectdiscovery/dnsx) A fast and flexible DNS toolkit for running DNS queries. It can resolve hostnames, filter responses, and validate records, making it useful for verifying subdomain discoveries.  
**Use Cases:**

- Resolving subdomains found via Subfinder to check if they are alive.
- Performing DNS record lookups (A, CNAME, TXT, MX, etc.).
- Filtering valid domains from a large list.

```bash { name=dnsx }
OUT="$OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl"
mkdir -p "$(dirname "$OUT")"

jq -r 'select(.host!=null) | .host' $OPSFOLIO_EAA_HOME/subfinder/subfinder.jsonl \
  | sed 's/ .*//' \
  | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
  | dnsx -json -silent -o "$OUT" || true
```

Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl` containing resolved IPs, CNAMEs, and other DNS data.

# httpx-toolkit – Probing Web Services

[httpx-toolkit](https://www.kali.org/tools/httpx-toolkit/) A fast HTTP toolkit that probes web servers to collect information such as status codes, titles, technologies, TLS details, redirects, and response headers. It helps identify live hosts and gather intelligence.  
**Use Cases:**

- Checking which discovered subdomains are live.
- Collecting metadata (status codes, titles, headers, TLS details).
- Identifying web technologies for further exploitation.

```bash { name=httpx }
OUT="$OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl"

# Create the output directory if it does not exist
mkdir -p "$(dirname "$OUT")"

jq -r .host $OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl \
  | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
  > targets.txt || true

cat "$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt" >> targets.txt || true

httpx-toolkit -l targets.txt -json -silent \
  -rl "${OPSFOLIO_EAA_RATE_LIMIT:-200}" \
  -threads "${OPSFOLIO_EAA_CONCURRENCY:-50}" \
  -o "$OUT"

```

Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/httpx/httpx-toolkit.jsonl` with fields like URL, status code, title, webserver header.

# WhatWeb – Fingerprinting Technologies

[WhatWeb](https://github.com/urbanadventurer/WhatWeb) A web scanner that identifies websites’ technologies, frameworks, CMS, server details, and other metadata. It’s useful for fingerprinting applications during reconnaissance.  
**Use Cases:**

- Detecting CMS (e.g., WordPress, Joomla, Drupal) in use.
- Identifying server-side technologies (Apache, Nginx, PHP, etc.).
- Profiling web applications for potential vulnerabilities.

```bash { name=whatweb }
mkdir -p "$OPSFOLIO_EAA_HOME/whatweb"

# Extract URLs, remove excluded ones, remove duplicates
jq -r .url "$OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl" \
  | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
  | sort -u \
  | while read -r url; do
      safe=$(echo "$url" | sed 's#[/:?&=]#_#g')
      out="$OPSFOLIO_EAA_HOME/whatweb/$safe.json"

      # Only run if this output file hasn’t been created yet
      if [ ! -s "$out" ]; then
        whatweb --log-json="$out" "$url" || true
      fi
    done
```

Artifacts: per-target JSON files under `$OPSFOLIO_EAA_HOME/whatweb/whatweb.jsonl` with detected technologies.
The way to detect anomalies using WhatWeb findings include:

- Refer [whatweb-security-engineer.ctxe.md](./whatweb-security-engineer.ctxe.md)

# Naabu – Scanning Open Ports

[Naabu](https://github.com/projectdiscovery/naabu) A fast port scanner written in Go. It can scan large IP ranges to identify open ports, serving as a lightweight and high-performance alternative to traditional scanners.  
**Use Cases:**

- Discovering open ports on a target system.
- Identifying exposed services (HTTP, SSH, FTP, etc.).
- Feeding live ports into service enumeration tools like Nmap.

```bash { name=naabu }
mkdir -p $OPSFOLIO_EAA_HOME/naabu

jq -r .host $OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl \
  | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
  > naabu_hosts.txt || true

naabu -list naabu_hosts.txt -json -silent \
  -top-ports "${OPSFOLIO_EAA_NAABU_PORTS#top-}" \
  -rate "${OPSFOLIO_EAA_RATE_LIMIT:-200}" \
  -c "${OPSFOLIO_EAA_CONCURRENCY:-50}" \
  -o $OPSFOLIO_EAA_HOME/naabu/naabu.jsonl

```

Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/naabu/naabu.jsonl` with IP and port fields.

# Nmap – Service Enumeration

[Nmap](https://nmap.org/) One of the most popular and versatile network scanning tools. It detects open ports, services, versions, and even operating systems on target systems.  
**Use Cases:**

- Performing comprehensive port scanning and service detection.
- Detecting operating systems and service versions.
- Running vulnerability detection scripts (via NSE).

```bash { name=nmap }
# Ensure output directory exists
mkdir -p "$OPSFOLIO_EAA_HOME/nmap"

# Extract unique IPs from naabu.jsonl into a temporary file
TARGETS_FILE="$OPSFOLIO_EAA_HOME/nmap/nmap_targets.txt"
jq -r .ip "$OPSFOLIO_EAA_HOME/naabu/naabu.jsonl" | sort -u > "$TARGETS_FILE"

# Run nmap with aggressive scan (-A), no ping (-Pn), faster timing (-T4)
# Output in normal format (-oN)
nmap -A -Pn -T4 -oX "$OPSFOLIO_EAA_HOME/nmap/nmap.xml" -iL "$TARGETS_FILE" || true

# Remove the temporary targets file
rm -f "$TARGETS_FILE"
```

Artifacts: XML and JSON representations of Nmap results at `$OPSFOLIO_EAA_HOME/nmap/nmap.*`.

# OpenSSL – Inspecting TLS Certificates

[OpenSSL](https://www.openssl.org/) A robust toolkit for the Transport Layer Security (TLS) and Secure Sockets Layer (SSL) protocols. It’s used to generate and manage keys/certificates, test SSL connections, and troubleshoot cryptographic issues.  
**Use Cases:**

- Generating SSL/TLS certificates for secure communication.
- Testing SSL handshakes and debugging HTTPS issues.
- Checking for weak or expired certificates.

```bash { name=openssl }
# Ensure directory exists
mkdir -p $OPSFOLIO_EAA_HOME/openssl

for host in $(jq -r .host $OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl \
              | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt"); do
  safe=$(echo "$host" | sed 's#[/:]#_#g')   # sanitize filename
  echo | openssl s_client -servername "$host" -connect "$host:443" -showcerts 2>/dev/null \
        > "$OPSFOLIO_EAA_HOME/openssl/$safe.txt" || true
done
```

Artifacts: plain text certificate details in `$OPSFOLIO_EAA_HOME/openssl/*.txt`.

# Nuclei – Template-Based Vulnerability Scanning

[Nuclei](https://github.com/projectdiscovery/nuclei) A fast vulnerability scanner that uses community-contributed templates to detect misconfigurations, CVEs, exposures, and other security issues. It automates large-scale scanning with customizable templates.  
**Use Cases:**

- Scanning web apps for known CVEs using templates.
- Detecting misconfigurations (e.g., exposed panels, default creds).
- Automating bug bounty reconnaissance workflows.

```bash { name=nuclei }
# Ensure output directory exists
mkdir -p "$OPSFOLIO_EAA_HOME/nuclei"

# Extract only the hostnames from httpx results and save to temporary targets file
jq -r .url "$OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl" \
  | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
  | sed -E 's,https?://([^/:]+).*,\1,' \
  | sort -u \
  > "$OPSFOLIO_EAA_HOME/nuclei/nuclei_targets.txt" || true

# Run nuclei against the extracted domains
nuclei -list "$OPSFOLIO_EAA_HOME/nuclei/nuclei_targets.txt" \
  -rate-limit "${OPSFOLIO_EAA_RATE_LIMIT:-200}" \
  -c "${OPSFOLIO_EAA_CONCURRENCY:-50}" \
  -jsonl-export "$OPSFOLIO_EAA_HOME/nuclei/nuclei.jsonl" || true

# Remove temporary targets file
rm -f "$OPSFOLIO_EAA_HOME/nuclei/nuclei_targets.txt"
```

Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/nuclei/nuclei.jsonl` with one result per finding.

# Katana – Crawling for Endpoints

[Katana](https://github.com/projectdiscovery/katana) A powerful web crawling tool designed to discover hidden files, endpoints, and parameters. It supports modern web technologies (like JS parsing) and is useful for application mapping and content discovery.  
**Use Cases:**

- Crawling target websites to find hidden endpoints.
- Extracting URLs and parameters for fuzzing.
- Mapping web applications for deeper testing.

```bash { name=katana }
if command -v katana >/dev/null; then
  jq -r .url $OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl \
    | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
    > katana_targets.txt || true

  # Ensure output directory exists
  mkdir -p $OPSFOLIO_EAA_HOME/katana

  katana -list katana_targets.txt -jsonl -o $OPSFOLIO_EAA_HOME/katana/katana.jsonl || true
fi
```

Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/katana/katana.jsonl` listing discovered endpoints.

# tlsx – TLS Metadata Extraction

[tlsx](https://github.com/projectdiscovery/tlsx) A TLS/SSL scanner that helps analyze SSL certificates, extract metadata, and check for security issues in TLS configurations. It is valuable for identifying weak or misconfigured SSL setups.  
**Use Cases:**

- Extracting SSL certificate details from multiple hosts.
- Identifying weak TLS versions or cipher suites.
- Monitoring certificate expiration across domains.

```bash { name=tlsx }
if command -v tlsx >/dev/null; then
  # Ensure output directory exists
  mkdir -p $OPSFOLIO_EAA_HOME/tlsx

  # Write targets inside the same folder
  jq -r .host $OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl \
    | grep -v -F -f "$OPSFOLIO_EAA_SESSION_HOME/excludes.txt" \
    > $OPSFOLIO_EAA_HOME/tlsx/tlsx_targets.txt || true

  # Only run if we have targets
  if [ -s $OPSFOLIO_EAA_HOME/tlsx/tlsx_targets.txt ]; then
    tlsx -list $OPSFOLIO_EAA_HOME/tlsx/tlsx_targets.txt \
         -silent  \
         -json -o $OPSFOLIO_EAA_HOME/tlsx/tlsx.jsonl || true
  else
    echo "[INFO] No TLSX targets found"
  fi
# Remove services.xml after conversion
rm -f $OPSFOLIO_EAA_HOME/tlsx/tlsx_targets.txt
fi
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/tlsx/tlsx.jsonl` with structured TLS details.

# Dirsearch – Directory Enumeration

[Dirsearch](https://github.com/projectdiscovery/tlsx) Dirsearch is an open-source command-line tool used for brute-forcing directories and files on web servers. It helps security testers and administrators discover hidden resources, misconfigured files, and sensitive endpoints that are not publicly linked. The tool supports multithreading, custom wordlists, recursive scans, proxy support, and can handle various HTTP methods, making it efficient for web application reconnaissance and vulnerability assessments.

Use Cases

- Hidden Path Discovery
- Exposure of Misconfigurations
- Recon in Pentesting

```bash { name=dirsearch }
if command -v dirsearch >/dev/null; then
  OUT="$OPSFOLIO_EAA_HOME/dirsearch/dirsearch.json"
  mkdir -p "$(dirname "$OUT")"

  # Create key_urls.txt from env variable
  KEY_URLS_FILE="$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt"
  mkdir -p "$(dirname "$KEY_URLS_FILE")"
  echo "$OPSFOLIO_EAA_KEY_URLS" | tr ',' '\n' | tr ' ' '\n' > "$KEY_URLS_FILE"

  # Run dirsearch directly on key_urls.txt
  dirsearch -l "$KEY_URLS_FILE" -i 200 --format=json -o "$OUT" >/dev/null 2>&1 || true
fi
```

- Artifacts: JSONL file at `/$OPSFOLIO_EAA_HOME/dirsearch/dirsearch.jsonl` with structured TLS details.

# wafw00f – Fingerprints Web Application Firewall (WAF)

[wafw00f](https://www.kali.org/tools/wafw00f/) WAFW00F is a Web Application Firewall (WAF) fingerprinting tool. It helps security testers and penetration testers detect whether a website is protected by a WAF and, if so, identify the specific vendor or technology in use. It works by sending crafted HTTP requests and analyzing responses to determine patterns that match known WAF behaviors.

Use Cases

- Identify if a target application is protected by a WAF so that penetration testers can adjust their testing approach accordingly.
- Determine the specific WAF vendor (e.g., Cloudflare, Akamai, Imperva) to understand its protection mechanisms and known bypass techniques.
- Validate whether an organization has correctly deployed a WAF as part of regulatory compliance (e.g., PCI DSS) or general security hardening.

```bash { name=wafw00f }
IN="$OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl"
OUT="$OPSFOLIO_EAA_HOME/wafw00f/wafw00f.txt"

mkdir -p "$(dirname "$OUT")"

# Run wafw00f on resolved hosts with timeout
jq -r 'select(.host!=null) | .host' "$IN" \
  | sort -u \
  | while read -r host; do
      wafw00f -a --no-colors "$host" || true
    done > "$OUT" || true
```

- Artifacts: JSONL file at `/var/fleetfolio/eaa/wafw00f/wafw00f.jsonl` with structured TLS details.

# Testssl – SSL/TLS Security Testing Tool

[Testssl](https://github.com/testssl/testssl.sh) Testssl.sh is an open-source command-line tool used to test SSL/TLS configurations of servers.

Use Cases

- Detect weak or deprecated SSL/TLS protocols and ciphers.
- Identify SSL/TLS misconfigurations (e.g., insecure renegotiation, Heartbleed).

```bash { name=testssl }
IN="$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt"
OUT="$OPSFOLIO_EAA_HOME/testssl/testssl.json"

mkdir -p "$(dirname "$OUT")"

# Corrected syntax
testssl --file "$IN" --jsonfile-pretty "$OUT"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/testssl/testssl.jsonl` with structured TLS details.

# Sqlmap – SSL/TLS Security Testing Tool

[Sqlmap](https://github.com/sqlmapproject/sqlmap) is an open-source penetration testing tool that automates the process of detecting and exploiting SQL injection vulnerabilities in web applications. It supports a wide range of databases and can help security testers identify and confirm database flaws quickly.

Use Cases

- Detect SQL Injection – Automatically test web applications for SQL injection vulnerabilities.
- Database Enumeration – Extract database names, tables, columns, and even data from vulnerable applications.
- Privilege Escalation & Access – Identify database users, check privileges, and attempt to gain administrative access to the backend.

```bash { name=sqlmap }
IN="$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt"
OUT="$OPSFOLIO_EAA_HOME/sqlmap/sqlmap.txt"

mkdir -p "$(dirname "$OUT")"

# Run sqlmap on each URL and append results to OUT
cat "$IN" | sort -u | while read -r url; do
    {
        echo "=================================================="
        echo "[*] Testing URL: $url"
        echo "=================================================="
        sqlmap --random-agent --batch \
               --current-user --current-db \
               -u "$url" || true
        echo
    } >> "$OUT"
done
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/sqlmap/sqlmap.txt` with structured TLS details.

# Subzy – Subdomain takeover

[Subzy](https://github.com/PentestPad/subzy) is an open-source tool used to detect and exploit subdomain takeover vulnerabilities. It scans for misconfigured subdomains pointing to deprovisioned services which works based on matching response fingerprints from [can-i-take-over-xyz](https://github.com/EdOverflow/can-i-take-over-xyz/blob/master/README.md).

Use Cases

- Identify subdomains vulnerable to takeover due to misconfigured DNS records.
- Automate reconnaissance during bug bounty or penetration testing.
- Prevent security risks by monitoring and fixing dangling subdomains in an organization.

```bash { name=subzy }
IN="$OPSFOLIO_EAA_HOME/subfinder/subfinder.jsonl"
TXT_OUT="$OPSFOLIO_EAA_HOME/subzy/subzy-temp.txt"
OUT="$OPSFOLIO_EAA_HOME/subzy/subzy.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Extract subdomains → save to TXT
jq -r '.host' "$IN" | sort -u > "$TXT_OUT"

# Run subzy → strip ANSI colors → save clean output
subzy run --targets "$TXT_OUT" \
  | sed -r "s/\x1B\[[0-9;]*[mK]//g" \
  > "$OUT"

# Remove TXT file after use
rm -f "$TXT_OUT"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/subzy/subzy.txt` with structured TLS details.

# Corsy – Cross Origin Resource Sharing Misconfiguration

[Corsy](https://github.com/s0md3v/Corsy/tree/master) is a lightweight program that scans for all known misconfigurations in CORS implementations.

Use Cases

- Detecting Insecure Wildcard Origins
- Identifying Trusted Subdomain Exploits
- Auditing Sensitive Endpoints for Data Leakage

```bash { name=corsy }
IN="$OPSFOLIO_EAA_HOME/httpx-toolkit/httpx-toolkit.jsonl"
OUT="$OPSFOLIO_EAA_HOME/corsy/corsy.txt"
TMP="$OPSFOLIO_EAA_HOME/corsy/corsy-tmp.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Check if the input file is empty or exists
if [ ! -s "$IN" ]; then
  echo "❌ Input file $IN is empty or does not exist."
  exit 1
fi

# Extract URLs silently
jq -r '.input as $host | .scheme as $scheme | "\($scheme)://\($host)"' "$IN" \
  | sed -E 's,:(80|443)(/|$),\2,' \
  | sort -u \
  > "$TMP"

# Validate temporary file
if [ ! -s "$TMP" ]; then
  echo "❌ No data extracted from $IN. Check jq/sed."
  exit 1
fi

# Install dependencies silently
pip3 install -r "$OPSFOLIO_EAA_CORSY_PATH/requirements.txt" --break-system-packages \
  > /dev/null 2>&1 || { echo "❌ pip install failed"; exit 1; }

# Run Corsy silently
python3 "$OPSFOLIO_EAA_CORSY_PATH/corsy.py" -i "$TMP" -t 20 -d 2 > "$OUT" 2>/dev/null \
  || { echo "❌ Corsy script failed"; exit 1; }

# Verify output file
if [ ! -s "$OUT" ]; then
  echo "⚠️ Output file $OUT is empty. No results found."
  exit 1
fi

# Cleanup
rm -f "$TMP"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/corsy/corsy.txt` with structured TLS details.

# Nikto – Web server and CGI scanner

[Nikto](https://www.kali.org/tools/nikto/) is a pluggable web server and CGI scanner written in Perl, using rfp’s LibWhisker to perform fast security or informational checks.

Use Cases

- Available HTTP versions automatic switching
- Generic as well as specific server software checks
- SSL support (through libnet-ssleay-perl)

```bash { name=nikto }
OUT="$OPSFOLIO_EAA_HOME/nikto/nikto.json"
IN="$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt"
# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"
# Run nikto
nikto --host "$IN" -output "$OUT"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/nikto/nikto.txt` with structured TLS details.

# WPScan – WordPress Security Scanner

[WPScan](https://www.kali.org/tools/wpscan/) target WordPress URL and enumerate any plugins that are installed.

Use Cases

- Available HTTP versions automatic switching
- Generic as well as specific server software checks
- SSL support (through libnet-ssleay-perl)

```bash { name=wpscan }
OUT="$OPSFOLIO_EAA_HOME/wpscan/wpscan.json"
IN="$OPSFOLIO_EAA_SESSION_HOME/key_urls.txt"
TOKEN="$OPSFOLIO_EAA_SESSION_HOME/wp_token.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Read the first URL from the file (remove any trailing spaces)
URL=$(head -n1 "$IN" | tr -d '[:space:]')

# Read the API token (remove any trailing spaces)
API_TOKEN=$(cat "$TOKEN" | tr -d '[:space:]')

# Run WPScan
wpscan --url "$URL" --api-token "$API_TOKEN" --enumerate u --format JSON -o "$OUT"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/wpscan/wpscan.json` with structured TLS details.

# RustScan – Modern Port Scanner

[RustScan](https://github.com/bee-san/RustScan) is a modern take on the port scanner. Sleek & fast. All while providing extensive extendability to you.

Use Cases

- Scans all 65k ports in 3 seconds.
- Full scripting engine support. Automatically pipe results into Nmap, or use our scripts (or write your own) to do whatever you want.
- Adaptive learning. RustScan improves the more you use it. No bloated machine learning here, just basic maths.

```bash { name=rustscan }
OUT="$OPSFOLIO_EAA_HOME/rustscan/rustscan.txt"
IN="$OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl"
TEMP="$OPSFOLIO_EAA_HOME/rustscan/rustscan-temp.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Extract hostnames into a temp file
jq -r '.host' "$IN" > "$TEMP"

# Run rustscan with host list
rustscan -a "$TEMP" --ulimit 5000 -t 2000 > "$OUT"

# Clean up temp file
rm -f "$TEMP"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/rustscan/rustscan.txt` with structured TLS details.

# Amass – Network Mapping

[Amass](https://github.com/bee-san/RustScan)  performs network mapping of attack surfaces and external asset discovery using open source information gathering and active reconnaissance techniques.

Use Cases

- Subdomain Enumeration
- Attack Surface Discovery and Mapping
- External Asset Inventory and Monitoring

```bash { name=amass }
OUT="$OPSFOLIO_EAA_HOME/amass/amass.txt"
IN="$OPSFOLIO_EAA_SESSION_HOME/domains.txt"

mkdir -p "$(dirname "$OUT")"

while read -r domain; do
    amass enum -d "$domain" -o "$OUT"
done < "$IN"

```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/amass/amass.txt` with structured TLS details.

# DNSEnum – Network Mapping

[DNSEnum](https://www.kali.org/tools/dnsenum/) is a multithreaded perl script to enumerate DNS information of a domain and to discover non-contiguous ip blocks. The main purpose of Dnsenum is to gather as much information as possible about a domain.

Use Cases

- Discover hosts, subdomains, and DNS records (A, MX, NS, zone transfers).
- Perform network enumeration and lookups (C-class ranges, whois, reverse DNS).
- Save and document discovered IPs and assets.

```bash { name=dnsenum }
OUT="$OPSFOLIO_EAA_HOME/dnsenum/dnsenum.xml"
IN="$OPSFOLIO_EAA_SESSION_HOME/domains.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Run dnsenum for each domain and append output
> "$OUT"  # Clear output file
while read -r domain; do

  dnsenum "$domain" -o "$OUT" \
    > /dev/null 2>&1 || echo "⚠️ dnsenum failed for $domain"
done < "$IN"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/dnsenum/dnsenum.txt` with structured TLS details.

# TheHarvester – Network Mapping

[TheHarvester](https://www.kali.org/tools/theharvester/) is a tool for gathering subdomain names, e-mail addresses, virtual hosts, open ports/ banners, and employee names from different public sources (search engines, pgp key servers).

Use Cases

- Email and Employee Harvesting
- Subdomain and Host Discovery
- Metadata and Open-Source Intelligence Collection

```bash { name=theharvester }
OUT="$OPSFOLIO_EAA_HOME/theharvester/theharvester.json"
IN="$OPSFOLIO_EAA_SESSION_HOME/domains.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Validate input file
if [ ! -s "$IN" ]; then
  echo "❌ Input file $IN is empty or missing."
  exit 1
fi

echo "►  Running task theharvester..."

# Empty output file before starting
> "$OUT"

# Loop through each domain
while IFS= read -r domain; do
  # Skip empty/commented lines
  case "$domain" in
    ""|\#*) continue ;;
  esac

  echo "Scanning $domain..."
  theHarvester -d "$domain" -b otx -f "$OUT" > /dev/null 2>&1 || echo "⚠️ theHarvester failed for $domain"

done < "$IN"

echo "►  ✓ Task theharvester exited with code 0"
echo "Task completed. Output stored in $OUT."

```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/theharvesterum/theharvester.json` with structured TLS details.

# ParamSpider – Automating advanced SQL injection attacks

[Paramspider](https://www.kali.org/tools/theharvester/) allows you to fetch URLs related to any domain or a list of domains from Wayback achives. It filters out "boring" URLs, allowing you to focus on the ones that matter the most.

Use Cases

- Discover hidden URL parameters
- Expand attack surface
- Support automated vulnerability testing

```bash { name=paramspider }
OUT="$OPSFOLIO_EAA_HOME/paramspider/paramspider.txt"
IN="$OPSFOLIO_EAA_SESSION_HOME/domains.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

# Loop through each domain in the input file
while read -r domain; do
     timeout 300 paramspider -d "$domain" >> "$OUT" 2>/dev/null
done < "$IN"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/paramspider/paramspider.txt` with structured TLS details.

# Ghauri – Automating advanced SQL injection attacks

[Ghauri](https://www.kali.org/tools/theharvester/) is an advanced cross-platform tool that automates the process of detecting and exploiting SQL injection security flaws.It supports multiple injection types (boolean, error-based, time-based, stacked queries, etc.) and works with various databases like MySQL, PostgreSQL, MSSQL, Oracle, etc. It also supports attacking via GET/POST, headers, JSON, SOAP/XML, allows using proxies, custom payloads, skip encoding, session resume, and more.

Use Cases

- Identify SQL injection vulnerabilities
- Enumerate database structure and extract sensitive data
- Leverage SQL injection for privilege escalation

```bash { name=Ghauri }
OUT="$OPSFOLIO_EAA_HOME/ghauri/ghauri.txt"
IN="$OPSFOLIO_EAA_HOME/paramspider/paramspider.txt"

# Ensure output directory exists
mkdir -p "$(dirname "$OUT")"

ghauri -m "$IN" --batch --current-user --current-db --hostname > "$OUT"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/ghauri/ghauri.txt` with structured TLS details.

# cdncheck – CDN Detection Tool

[cdncheck](https://github.com/projectdiscovery/cdncheck) is a command-line utility developed by ProjectDiscovery designed to identify whether a given IP address belongs to known Content Delivery Network (CDN) providers, such as Akamai, Cloudflare, Incapsula, Sucuri, and Leaseweb.
Use Cases

- Check if target is behind a CDN.
- Understand infrastructure for risk evaluation.
- Trace traffic origin and content distribution during breaches.

```bash { name=cdncheck }
OUT="$OPSFOLIO_EAA_HOME/cdncheck/cdncheck.jsonl"
IN="$OPSFOLIO_EAA_HOME/dnsx/dnsx.jsonl"
CONCURRENCY=10   # adjust parallel workers as needed

mkdir -p "$(dirname "$OUT")"
> "$OUT"

tmpdir=$(mktemp -d)
trap 'rm -rf "$tmpdir"' EXIT

# Run cdncheck in parallel, each job writes to its own temp file
jq -r '.host' "$IN" | xargs -n1 -P "$CONCURRENCY" -I {} sh -c \
  'f=$(mktemp "$0/cdncheck.XXXXXX"); cdncheck -i "$1" -j > "$f"' "$tmpdir" {}

# Merge temp outputs into final jsonl (preserves one JSON object per line)
cat "$tmpdir"/cdncheck.* >> "$OUT"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/cdncheck/cdncheck.jsonl` with structured TLS details.

# FFUF – Fast Web Fuzzer

[FFUF](https://www.kali.org/tools/ffuf/) is a fast web fuzzer written in Go, used for discovering hidden files, directories, and parameters.
It supports fuzzing GET/POST data, virtual hosts, and headers with high performance. Commonly used in web penetration testing to identify undisclosed endpoints or functionalities.
Use Cases

- Hidden directory & file enumeration
- Virtual host (vhost) discovery via Host header fuzzing
- Parameter fuzzing for GET/POST endpoints

```bash { name=cdncheck }
OUT="$OPSFOLIO_EAA_HOME/ffuf/ffuf.json"
IN="$OPSFOLIO_EAA_SESSION_HOME/domains.txt"
WORDLIST="/usr/share/wordlists/seclists/Discovery/Web-Content/directory-list-1.0.txt"

mkdir -p "$(dirname "$OUT")"

while read -r domain; do
    ffuf -u "$IN"/FUZZ -w "$WORDLIST" -t 10 -p 0.2 -json -o "$OUT"
done < "$IN"
```

- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/ffuf/ffuf.json` with structured TLS details.

# Dalfox – Fast Web Fuzzer

[Dalfox](https://github.com/hahwul/dalfox) is a powerful open-source tool that focuses on automation, making it ideal for quickly scanning for XSS flaws and analyzing parameters. Its advanced testing engine and niche features are designed to streamline the process of detecting and verifying vulnerabilities.

Use Cases

- Discovery: Parameter analysis, static analysis, BAV testing, parameter mining
- XSS Scanning: Reflected, Stored, DOM-based, with optimization and DOM/headless verification
- HTTP Options: Custom headers, cookies, methods, proxy, and more

```bash { name=dalfox }
OUT="$OPSFOLIO_EAA_HOME/dalfox/dalfox.jsonl"
IN="$OPSFOLIO_EAA_HOME/paramspider/paramspider.txt"

mkdir -p "$(dirname "$OUT")"

dalfox file "$IN" --format jsonl -o "$OUT"
```
- Artifacts: JSONL file at `$OPSFOLIO_EAA_HOME/dalfox/dalfox.jsonl` with structured TLS details.
  
# Convert Results into SQLite Database with Surveilr

After the scanning process, the collected results are normalized and stored in a single SQLite database file named resource-surveillance.sqlite.db. This step ensures that all findings are structured, queryable, and ready for further analysis or reporting through Surveilr.

```bash { name=surveilr }
IN="$OPSFOLIO_EAA_HOME"
OUT="$OPSFOLIO_EAA_HOME"

# Extract the timestamp from the session directory name
timestamp=$(basename "$OPSFOLIO_EAA_HOME")

surveilr ingest files -r . \
  --tenant-id "$OPSFOLIO_EAA_TENANT_ID" \
  --tenant-name "$OPSFOLIO_EAA_TENANT_NAME" \
  --state-db-fs-path "$OUT/$OPSFOLIO_EAA_TENANT_ID.$timestamp.opsfolio-eaa-rssd.sqlite.db"

 zip -r "$OUT/$OPSFOLIO_EAA_TENANT_ID.$timestamp.opsfolio-eaa-rssd.sqlite.db".zip "$OUT/$OPSFOLIO_EAA_TENANT_ID.$timestamp.opsfolio-eaa-rssd.sqlite.db" 
```

Artifacts: Convert Results into SQLite Database with Surveilr.

# Analyst’s Guide to Interpreting Artifacts

This appendix provides practical advice for reviewing the artifacts generated by each tool. It is meant for analysts who may
not be familiar with the nuances of every tool but need to make sense of the outputs.

## Subfinder (`subfinder.jsonl`)

Each line is a JSON object with a `host` field and the `source` where it was discovered. Analysts should look for:

- Subdomains that are unexpected or unmanaged.
- Entries that do not resolve later in dnsx, which may indicate legacy or abandoned records.

## dnsx (`dnsx.jsonl`)

DNS resolution results will include IPs (A/AAAA records), CNAMEs, and other metadata. Analysts should:

- Verify that IPs map to owned infrastructure.
- Flag any pointing to cloud or third-party networks that might be unmanaged.

## httpx-toolkit (`httpx-toolkit.jsonl`)

Contains metadata for live web services: status codes, titles, server headers. Analysts should:

- Look for sensitive endpoints (e.g., admin panels, login portals).
- Pay attention to unusual server headers or technologies that don’t align with policy.

## WhatWeb (`whatweb/*.json`)

Per-target JSON files list detected plugins/technologies. Analysts should:

- Identify outdated CMS or frameworks (e.g., old WordPress, Joomla).
- Cross-check detected versions against known vulnerabilities.

## Naabu (`naabu.jsonl`)

JSON lines listing IPs and open ports. Analysts should:

- Spot unexpected open ports (e.g., databases, RDP, SSH exposed externally).
- Focus on high-risk services like SMB, Telnet, or legacy protocols.

## Nmap (`nmap.xml`)

Provides enriched service banners and versions. Analysts should:

- Confirm the accuracy of Naabu findings.
- Evaluate service versions for end-of-life software.

## OpenSSL (`tls/*.txt`)

Raw transcripts of TLS handshakes and certificate chains. Analysts should:

- Check expiry dates, SAN entries, and certificate issuers.
- Flag self-signed or weakly signed certificates.

## Nuclei (`nuclei.txt`)

Each JSON line represents a finding matched against a template. Analysts should:

- Sort by severity to prioritize triage.
- Validate important findings manually before reporting.

## Katana (`katana.jsonl`)

Lists discovered web endpoints through crawling. Analysts should:

- Look for API endpoints, hidden admin paths, or sensitive resources.
- Cross-reference endpoints against vulnerability scan coverage.

## Dirsearch (`dirsearch.json`)

Structured Dirsearch scan results including discovered directories, files, and HTTP response codes. Analysts should:

- Use this to identify hidden or sensitive directories and files on the target.
- Review discovered paths for potential misconfigurations or exposure of sensitive data.

## wafw00f (`wafw00f.txt`)

Structured WAFW00F results identifying web application firewalls (WAFs) deployed on the target. Analysts should:

- Use this to understand WAF presence and tailor payloads or bypass strategies.
- Document detected WAF vendors and signatures for future reference.

## Testssl (`testssl.json`)

Structured TestSSL scan results including TLS configurations, supported cipher suites, and protocol details. Analysts should:

- Use this to identify weak ciphers, outdated SSL/TLS versions, or certificate misconfigurations.
- Review for compliance with current security best practices.

## Sqlmap (`sqlmap.txt`)

Structured SQLMap scan output containing SQL injection findings and payloads. Analysts should:

- Use this to identify vulnerable parameters and confirm exploitation feasibility.
- Document injection types, DBMS details, and any data retrieved from the database.

## Subzy (`subzy.txt`)

Structured Subzy scan output listing potential subdomain takeover vulnerabilities. Analysts should:

- Use this to identify abandoned DNS records pointing to third-party services.
- Validate and remediate by removing or claiming vulnerable subdomains.

## Corsy (`corsy.txt`)

Structured Corsy scan results analyzing Cross-Origin Resource Sharing (CORS) configurations. Analysts should:

- Use this to detect misconfigured origins allowing unauthorized access.
- Review for wildcard (`*`) origins or reflection-based trust issues.

## Nikto (`nikto.txt`)

Structured Nikto scan output identifying outdated software, misconfigurations, and known vulnerabilities. Analysts should:

- Use this to complement manual web application assessments.
- Review flagged issues for patching or hardening opportunities.

## WPScan (`wpscan.json`)

Structured WPScan results including WordPress core, plugin, and theme vulnerabilities. Analysts should:

- Use this to track outdated components and known CVEs.
- Recommend upgrades or removal of vulnerable extensions.

## RustScan (`rustscan.txt`)

Structured RustScan results enumerating open ports and responsive services. Analysts should:

- Use this to accelerate network reconnaissance with fast port discovery.
- Integrate with Nmap or other tools for detailed service enumeration.

## Amass (`amass.txt`)

Structured Amass output listing discovered subdomains and DNS relationships. Analysts should:

- Use this for comprehensive subdomain enumeration and asset discovery.
- Correlate with other recon tools to build a full attack surface map.

## DNSEnum (`dnsenum.xml`)

Structured DNSEnum results providing DNS record enumeration and zone transfer attempts. Analysts should:

- Use this to gather A, MX, NS, TXT, and CNAME records.
- Identify potential misconfigurations or information disclosure in DNS.

## TheHarvester (`theharvester.json`)

Structured TheHarvester results aggregating OSINT data such as emails, subdomains, and hosts. Analysts should:

- Use this to collect reconnaissance data from public sources.
- Cross-check discovered assets and contacts with internal inventories.

## Paramspider (`paramspider.txt`)

Structured ParamSpider output listing URLs and parameters for input-based vulnerabilities. Analysts should:

- Use this to feed into XSS, SSRF, and injection scanners.
- Review unique parameters for manual testing.

## Ghauri (`ghauri.txt`)

Structured Ghauri results identifying SQL injection vulnerabilities using advanced techniques. Analysts should:

- Use this for in-depth SQLi detection and database fingerprinting.
- Review payloads and confirm exploitation paths.

## cdncheck (`cdncheck.jsonl`)

Structured CDNCheck results identifying content delivery networks (CDNs) in use. Analysts should:

- Use this to detect CDN providers and confirm if assets are cached or protected.
- Review for exposed origin IPs that may bypass CDN protections.

## ffuf (`ffuf.json`)

Structured FFUF results including discovered endpoints, parameters, and response codes. Analysts should:

- Use this to map hidden directories, files, and API endpoints.
- Review status codes and responses for unauthorized access or sensitive data exposure.

## Dalfox (`Dalfox.jsonl`)

Structured Dalfox scan results including detected XSS payloads, reflection points, and scanning metadata. Analysts should:

- Use this to complement manual XSS testing with machine-readable JSONL data.
- Review findings for confirmed and potential XSS vectors.

---

This guide should be used alongside the summary step to quickly assess which artifacts need deeper analysis.
